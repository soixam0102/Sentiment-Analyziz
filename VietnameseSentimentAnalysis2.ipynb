{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dfb71908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<>': 1, 'mọi': 2, 'ngả': 3, 'đường': 4, 'vào': 5, 'tp': 6, 'hcm': 7, 'ngày': 8, 'lực_lượng': 9, 'của': 10, 'từ': 11, 'thành_phố': 12, 'đóng_cửa': 13, 'chuyển': 14, 'gia_cầm': 15, '15': 16, '1': 17, 'thú_y': 18, 'phối_hợp': 19, 'với': 20, 'cảnh_sát': 21, 'thanh_tra': 22, 'giao_thông': 23, 'mở': 24, 'điểm': 25, 'kiểm_soát': 26, 'trên': 27, 'ngoại_ô': 28, 'thực_hiện': 29, 'công_điện': 30, 'bộ': 31, 'nông_nghiệp': 32, 'và': 33, 'phát_triển': 34, 'nông_thôn': 35, 'trước': 36, 'đó': 37, 'một': 38, 'chức_năng': 39, 'đã': 40, 'chặn': 41, 'không': 42, 'để': 43, 'con': 44, 'gà': 45, 'nào': 46, 'tỉnh': 47, 'ngoài': 48, 'lọt': 49, 'tại': 50, 'trạm': 51, 'kiểm_dịch': 52, 'động_vật': 53, 'huyện': 54, 'bình_chánh': 55, 'cửa_ngõ': 56, 'phía': 57, 'tây': 58, 'phó_trạm': 59, 'phạm_ngọc_lanh': 60, 'mắt': 61, 'thâm_quầng': 62, 'nói': 63, '“': 64, 'khi': 65, 'xảy': 66, 'ra': 67, 'dịch': 68, 'đến': 69, 'giờ': 70, 'anh_em': 71, 'thay': 72, 'nhau': 73, 'túc_trực': 74, 'ở': 75, 'đây': 76, '…': 77}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from pyvi import ViTokenizer\n",
    "import os\n",
    "\n",
    "doc = \"Đóng cửa mọi ngả đường chuyển gia cầm vào TP HCM Ngày 15/1, lực lượng thú y phối hợp với cảnh sát, \" \\\n",
    "      \"thanh tra giao thông TP HCM mở điểm kiểm soát trên mọi ngả đường ngoại ô. \" \\\n",
    "      \"Thực hiện công điện của Bộ Nông nghiệp và phát triển nông thôn trước đó một ngày, lực lượng chức năng đã chặn không để con gà nào từ tỉnh ngoài lọt vào thành phố.\" \\\n",
    "      \"Tại Trạm Kiểm dịch động vật huyện Bình Chánh, cửa ngõ phía Tây của thành phố, Phó trạm Phạm Ngọc Lanh mắt thâm quầng nói: \" \\\n",
    "      \"“Từ khi xảy ra dịch đến giờ, anh em thay nhau túc trực ở đây….\"\n",
    "\n",
    "doc = ViTokenizer.tokenize(doc)\n",
    "doc = doc.lower()\n",
    "token = doc.split()\n",
    "table = str.maketrans('', '', string.punctuation.replace(\"_\", \"\"))\n",
    "token = [w.translate(table) for w in token]\n",
    "token = [word for word in token if word]\n",
    "# print(token)\n",
    "\n",
    "\n",
    "sequences = [\"xin chào hà nôi\"]\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[]^`{|}~ ', oov_token=\"<>\")\n",
    "tokenizer.fit_on_texts(token)\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf366213",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('myenv': conda)",
   "language": "python",
   "name": "python395jvsc74a57bd0d8f54f19d88e051e72ef3eb8c29534107549a0c7b24a7c8e60b74598e47a86c9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
